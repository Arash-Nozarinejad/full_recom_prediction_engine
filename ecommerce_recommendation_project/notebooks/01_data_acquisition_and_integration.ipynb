{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb753f85-7548-48fc-ba4f-2eac52aee064",
   "metadata": {},
   "source": [
    "# E-commerce Customer Behavior Prediction and Recommendation Engine\n",
    "## Notebook 1: Data Acquisition and Integration\n",
    "\n",
    "This notebook focuses on the critical first step: establishing a robust data acquisition and integration pipeline that will form the foundation for all subsequent analyses and modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3312573-432d-4f76-b189-53d3ff931cad",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before diving into data acquisition, let's set up our environment with the necessary libraries and configurations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ba0b28-38b6-4ad1-a46e-3b84f4f3ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import glob\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Determine the project root.\n",
    "# If running in a Jupyter notebook, assume the current working directory is 'notebooks'\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1] if '__file__' in globals() else Path.cwd().parent\n",
    "\n",
    "# Load environment variables from the .env file located in the project root\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure the logs directory exists (located at PROJECT_ROOT/logs)\n",
    "log_dir = PROJECT_ROOT / \"logs\"\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up logging to track our data acquisition process.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_dir / \"data_acquisition.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"DataAcquisition\")\n",
    "\n",
    "# Set display options for better readability of dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "# Set random seed for reproducibility across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ed0e6-38aa-4fb7-8f0e-89148ed96786",
   "metadata": {},
   "source": [
    "## Project Directory Structure\n",
    "\n",
    "Let's create a well-organized project structure to maintain clean separation between raw data, processed data, models, and outputs. This follows data science best practices and will make our project more maintainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87b147d-067f-4415-8703-441370a2d54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 12:34:20,356 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\data\\raw\n",
      "2025-03-26 12:34:20,358 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\data\\processed\n",
      "2025-03-26 12:34:20,359 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\data\\interim\n",
      "2025-03-26 12:34:20,361 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\models\n",
      "2025-03-26 12:34:20,363 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\outputs\n",
      "2025-03-26 12:34:20,364 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\logs\n",
      "2025-03-26 12:34:20,365 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\reports\n",
      "2025-03-26 12:34:20,367 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\notebooks\n",
      "2025-03-26 12:34:20,368 - DataAcquisition - INFO - Created directory: C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\configs\n"
     ]
    }
   ],
   "source": [
    "# Create project directory structure\n",
    "def create_project_structure(base_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a standardized project directory structure for organizing data and outputs.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: The base directory for the project (as a Path object)\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping directory names to their paths (as Path objects)\n",
    "    \"\"\"\n",
    "    directories = {\n",
    "        \"raw_data\": base_dir / \"data\" / \"raw\",           # Original, immutable data\n",
    "        \"processed_data\": base_dir / \"data\" / \"processed\", # Cleaned and transformed data\n",
    "        \"interim_data\": base_dir / \"data\" / \"interim\",     # Temporary data between processing steps\n",
    "        \"models\": base_dir / \"models\",                     # Trained models and model artifacts\n",
    "        \"outputs\": base_dir / \"outputs\",                   # Analysis outputs and visualizations\n",
    "        \"logs\": base_dir / \"logs\",                         # Logs from various processes\n",
    "        \"reports\": base_dir / \"reports\",                   # Generated analysis reports\n",
    "        \"notebooks\": base_dir / \"notebooks\",               # Jupyter notebooks\n",
    "        \"configs\": base_dir / \"configs\"                    # Configuration files\n",
    "    }\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    for name, dir_path in directories.items():\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Created directory: {dir_path}\")\n",
    "    \n",
    "    return directories\n",
    "\n",
    "# Use the previously defined PROJECT_ROOT from the earlier code block\n",
    "project_dirs = create_project_structure(PROJECT_ROOT)\n",
    "\n",
    "# Display the project structure\n",
    "# print(\"Project Directory Structure Created:\")\n",
    "# for name, path in project_dirs.items():\n",
    "    # print(f\"- {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92408d48-fabb-4577-b0fb-6b7335f71575",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "For this project, we'll be working with four complementary datasets that together provide a comprehensive view of customer behavior:\n",
    "\n",
    "| Dataset | Description | Size | Key Features |\n",
    "|---------|-------------|------|--------------|\n",
    "| **E-commerce User Behavior** | Tracks product views, cart additions, and purchases | ~14M events, ~70K users | user_id, product_id, event_type, price |\n",
    "| **Amazon Product Reviews** | Product reviews with ratings and metadata | Millions of reviews | product_id, customer_id, review_text, star_rating |\n",
    "| **Online Retail II** | Transactional data from UK-based retailer | ~1M transactions, ~4K customers | InvoiceNo, StockCode, Quantity, Price, CustomerID |\n",
    "| **Customer Personality** | Customer demographics and behaviors | ~2K customers | Age, Education, Income, Purchase history |\n",
    "\n",
    "Let's define the configuration for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83151a6-5fed-493f-b348-4dbbd197c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 12:34:28,090 - DataAcquisition - INFO - Saved dataset configuration to C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\configs/datasets_config.json\n"
     ]
    }
   ],
   "source": [
    "# Define dataset configurations\n",
    "datasets_config = {\n",
    "    \"ecommerce_behavior\": {\n",
    "        \"name\": \"E-commerce User Behavior Dataset\",\n",
    "        \"local_path\": f\"{project_dirs['raw_data']}/ecommerce_behavior\",\n",
    "        \"file_pattern\": \"*.csv\",\n",
    "        \"description\": \"User behavior data tracking product views, cart additions, and purchases\",\n",
    "        \"source\": \"https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\",\n",
    "        \"size_estimate\": \"~14 million events from ~70,000 users across 12 months\",\n",
    "        \"key_features\": [\"user_id\", \"product_id\", \"category_id\", \"event_type\", \"event_time\", \"price\", \"brand\"]\n",
    "    },\n",
    "    \"amazon_reviews\": {\n",
    "        \"name\": \"Amazon Product Reviews Dataset\",\n",
    "        \"local_path\": f\"{project_dirs['raw_data']}/amazon_reviews\",\n",
    "        \"file_pattern\": \"*.tsv\",\n",
    "        \"description\": \"Comprehensive product reviews with ratings and product metadata\",\n",
    "        \"source\": \"https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset\",\n",
    "        \"size_estimate\": \"Millions of reviews across multiple product categories\",\n",
    "        \"key_features\": [\"product_id\", \"customer_id\", \"review_text\", \"star_rating\", \"helpful_votes\", \"product_category\"]\n",
    "    },\n",
    "    \"online_retail\": {\n",
    "        \"name\": \"Online Retail II Dataset\",\n",
    "        \"local_path\": f\"{project_dirs['raw_data']}/online_retail\",\n",
    "        \"file_pattern\": \"*.xlsx\",\n",
    "        \"description\": \"Transactional data from a UK-based online retailer\",\n",
    "        \"source\": \"https://www.kaggle.com/datasets/mashlyn/online-retail-ii-uci\",\n",
    "        \"size_estimate\": \"~1 million transactions from ~4,000 customers over 2 years\",\n",
    "        \"key_features\": [\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\", \"CustomerID\", \"Country\"]\n",
    "    },\n",
    "    \"customer_personality\": {\n",
    "        \"name\": \"Customer Personality Analysis Dataset\",\n",
    "        \"local_path\": f\"{project_dirs['raw_data']}/customer_personality\",\n",
    "        \"file_pattern\": \"*.csv\",\n",
    "        \"description\": \"Detailed customer demographics and purchasing behaviors\",\n",
    "        \"source\": \"https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis\",\n",
    "        \"size_estimate\": \"~2,000 customers with 30+ demographic and behavioral features\",\n",
    "        \"key_features\": [\"Age\", \"Education\", \"Income\", \"Marital_Status\", \"Purchase history\", \"Campaign responses\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save dataset configuration to JSON for future reference\n",
    "with open(f\"{project_dirs['configs']}/datasets_config.json\", 'w') as f:\n",
    "    json.dump(datasets_config, f, indent=4)\n",
    "    \n",
    "logger.info(f\"Saved dataset configuration to {project_dirs['configs']}/datasets_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336eeccb-61c1-4367-8fd8-cb8272cec177",
   "metadata": {},
   "source": [
    "## Data Integration Architecture\n",
    "\n",
    "The diagram below outlines our data integration approach. We'll combine the four datasets through entity resolution and feature engineering to create a unified customer view that powers our recommendation and prediction models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3fe9841-5e08-418d-94b1-735e1cd57157",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '→' (U+2192) (868503627.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mData Sources → Preprocessing → Entity Resolution → Feature Engineering → Unified Data Layer → Models\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '→' (U+2192)\n"
     ]
    }
   ],
   "source": [
    "Data Sources → Preprocessing → Entity Resolution → Feature Engineering → Unified Data Layer → Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf2d6a-70f5-41f3-b24d-1543fd569918",
   "metadata": {},
   "source": [
    "For this project, we'll build a pipeline that integrates:\n",
    "- Customer demographics and profile information\n",
    "- Transaction history and purchase patterns\n",
    "- Product interactions (views, carts, purchases)\n",
    "- Product metadata and review sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a11a38-5e66-408f-9d6e-e7129b4a3824",
   "metadata": {},
   "source": [
    "## Data Loader Implementation\n",
    "\n",
    "Now, let's implement a robust `DataLoader` class that will handle loading, validation, and caching of our datasets. This class implements several important features for production-grade data processing:\n",
    "\n",
    "- **Configurable sampling**: For development and testing on smaller data subsets\n",
    "- **Data caching**: To speed up repeated processing\n",
    "- **Error handling**: Robust error detection and logging\n",
    "- **Data validation**: Basic checks to ensure data integrity\n",
    "- **Support for multiple file formats**: CSV, TSV, Excel, and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5381bf-8528-4902-9526-d2090b7639ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    A class to handle loading, basic preprocessing, and validation of datasets.\n",
    "    \n",
    "    Features:\n",
    "    - Configurable sampling for working with manageable data sizes\n",
    "    - Data caching to disk for faster reloads\n",
    "    - Support for multiple file formats (CSV, TSV, Excel)\n",
    "    - Robust error handling and logging\n",
    "    - File integrity validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasets_config: Dict[str, Dict[str, Any]], sample_size: Optional[int] = None, cache_enabled: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the DataLoader.\n",
    "        \n",
    "        Args:\n",
    "            datasets_config: Configuration dictionary for all datasets\n",
    "            sample_size: If provided, load only a sample of this size from each dataset\n",
    "            cache_enabled: Whether to use and create cache files for faster loading\n",
    "        \"\"\"\n",
    "        self.datasets_config = datasets_config\n",
    "        self.sample_size = sample_size\n",
    "        self.cache_enabled = cache_enabled\n",
    "        self.data_cache = {}  # Memory cache for loaded datasets\n",
    "        \n",
    "    def compute_file_hash(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Compute a hash of a file to track changes and ensure data integrity.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file\n",
    "            \n",
    "        Returns:\n",
    "            Hash string for the file\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            return \"\"\n",
    "            \n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "        \n",
    "    def load_csv(self, file_path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a CSV file with proper error handling.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the CSV file\n",
    "            **kwargs: Additional arguments to pass to pd.read_csv\n",
    "            \n",
    "        Returns:\n",
    "            Pandas DataFrame containing the data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading CSV file: {file_path}\")\n",
    "            \n",
    "            # Check for TSV file to handle Amazon reviews dataset\n",
    "            if file_path.endswith('.tsv'):\n",
    "                df = pd.read_csv(file_path, sep='\\t', **kwargs)\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, **kwargs)\n",
    "            \n",
    "            # Apply sampling if requested\n",
    "            if self.sample_size is not None and len(df) > self.sample_size:\n",
    "                df = df.sample(n=self.sample_size, random_state=42)\n",
    "                logger.info(f\"Sampled {self.sample_size:,} rows from {file_path}\")\n",
    "            \n",
    "            rows, cols = df.shape\n",
    "            logger.info(f\"Successfully loaded {file_path}: {rows:,} rows, {cols} columns\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading CSV file {file_path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def load_excel(self, file_path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load an Excel file with proper error handling.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the Excel file\n",
    "            **kwargs: Additional arguments to pass to pd.read_excel\n",
    "            \n",
    "        Returns:\n",
    "            Pandas DataFrame containing the data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading Excel file: {file_path}\")\n",
    "            \n",
    "            df = pd.read_excel(file_path, **kwargs)\n",
    "            \n",
    "            # Apply sampling if requested\n",
    "            if self.sample_size is not None and len(df) > self.sample_size:\n",
    "                df = df.sample(n=self.sample_size, random_state=42)\n",
    "                logger.info(f\"Sampled {self.sample_size:,} rows from {file_path}\")\n",
    "            \n",
    "            rows, cols = df.shape\n",
    "            logger.info(f\"Successfully loaded {file_path}: {rows:,} rows, {cols} columns\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Excel file {file_path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def load_parquet(self, file_path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a Parquet file with proper error handling.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the Parquet file\n",
    "            **kwargs: Additional arguments to pass to pd.read_parquet\n",
    "            \n",
    "        Returns:\n",
    "            Pandas DataFrame containing the data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading Parquet file: {file_path}\")\n",
    "            \n",
    "            df = pd.read_parquet(file_path, **kwargs)\n",
    "            \n",
    "            # Apply sampling if requested (unlikely needed for cached parquet, but included for consistency)\n",
    "            if self.sample_size is not None and len(df) > self.sample_size:\n",
    "                df = df.sample(n=self.sample_size, random_state=42)\n",
    "                logger.info(f\"Sampled {self.sample_size:,} rows from {file_path}\")\n",
    "            \n",
    "            rows, cols = df.shape\n",
    "            logger.info(f\"Successfully loaded {file_path}: {rows:,} rows, {cols} columns\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Parquet file {file_path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def validate_file_exists(self, file_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a file exists and log appropriate messages.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if the file exists\n",
    "        \"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            logger.info(f\"Found file: {file_path} ({file_size_mb:.2f} MB)\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f\"File not found: {file_path}\")\n",
    "            return False\n",
    "    \n",
    "    def get_dataset_files(self, dataset_id: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get all files matching the pattern for a dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: ID of the dataset\n",
    "            \n",
    "        Returns:\n",
    "            List of file paths\n",
    "        \"\"\"\n",
    "        if dataset_id not in self.datasets_config:\n",
    "            logger.error(f\"Unknown dataset ID: {dataset_id}\")\n",
    "            return []\n",
    "        \n",
    "        config = self.datasets_config[dataset_id]\n",
    "        pattern = os.path.join(config[\"local_path\"], config[\"file_pattern\"])\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        if not files:\n",
    "            logger.warning(f\"No files found matching pattern {pattern} for dataset {dataset_id}\")\n",
    "        \n",
    "        return files\n",
    "    \n",
    "    def check_cached_version(self, dataset_id: str, file_path: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Check if there's a cached version of a file and if it's still valid.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: ID of the dataset\n",
    "            file_path: Path to the original file\n",
    "            \n",
    "        Returns:\n",
    "            Path to the cached file if it exists and is valid, None otherwise\n",
    "        \"\"\"\n",
    "        if not self.cache_enabled:\n",
    "            return None\n",
    "            \n",
    "        cache_dir = f\"{project_dirs['interim_data']}/{dataset_id}_cache\"\n",
    "        base_name = os.path.basename(file_path)\n",
    "        cache_path = f\"{cache_dir}/{os.path.splitext(base_name)[0]}.parquet\"\n",
    "        \n",
    "        # Check if cache exists\n",
    "        if os.path.exists(cache_path):\n",
    "            # Compare modification times\n",
    "            orig_mtime = os.path.getmtime(file_path)\n",
    "            cache_mtime = os.path.getmtime(cache_path)\n",
    "            \n",
    "            if cache_mtime > orig_mtime:\n",
    "                logger.info(f\"Found valid cache for {file_path} at {cache_path}\")\n",
    "                return cache_path\n",
    "            else:\n",
    "                logger.info(f\"Cache exists but is outdated: {cache_path}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_to_cache(self, df: pd.DataFrame, dataset_id: str, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Save a DataFrame to cache for faster future loading.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to cache\n",
    "            dataset_id: ID of the dataset\n",
    "            file_path: Path to the original file\n",
    "            \n",
    "        Returns:\n",
    "            Path to the cached file\n",
    "        \"\"\"\n",
    "        if not self.cache_enabled:\n",
    "            return \"\"\n",
    "            \n",
    "        cache_dir = f\"{project_dirs['interim_data']}/{dataset_id}_cache\"\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        base_name = os.path.basename(file_path)\n",
    "        cache_path = f\"{cache_dir}/{os.path.splitext(base_name)[0]}.parquet\"\n",
    "        \n",
    "        logger.info(f\"Saving {len(df):,} rows to cache: {cache_path}\")\n",
    "        df.to_parquet(cache_path, index=False)\n",
    "        \n",
    "        return cache_path\n",
    "    \n",
    "    def load_file(self, dataset_id: str, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a single file, using cache if available.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: ID of the dataset\n",
    "            file_path: Path to the file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing the data\n",
    "        \"\"\"\n",
    "        # Check if file exists\n",
    "        if not self.validate_file_exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        # Check for cached version\n",
    "        cached_path = self.check_cached_version(dataset_id, file_path)\n",
    "        if cached_path:\n",
    "            return self.load_parquet(cached_path)\n",
    "        \n",
    "        # Load based on file extension\n",
    "        if file_path.endswith('.csv') or file_path.endswith('.tsv'):\n",
    "            df = self.load_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "            df = self.load_excel(file_path)\n",
    "        elif file_path.endswith('.parquet'):\n",
    "            df = self.load_parquet(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "        \n",
    "        # Save to cache for future use\n",
    "        if self.cache_enabled:\n",
    "            self.save_to_cache(df, dataset_id, file_path)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def validate_dataframe(self, df: pd.DataFrame, dataset_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Perform basic validation on a dataframe to ensure data integrity.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "            dataset_id: ID of the dataset for context\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if validation passed\n",
    "        \"\"\"\n",
    "        # Check if dataframe is empty\n",
    "        if df.empty:\n",
    "            logger.error(f\"DataFrame for {dataset_id} is empty\")\n",
    "            return False\n",
    "        \n",
    "        # Check for expected columns based on dataset config\n",
    "        expected_features = self.datasets_config[dataset_id][\"key_features\"]\n",
    "        missing_features = [f for f in expected_features if f not in df.columns and f.lower() not in df.columns]\n",
    "        \n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing expected features in {dataset_id}: {missing_features}\")\n",
    "            # We don't fail validation for this, just log a warning\n",
    "        \n",
    "        # Check for completely empty columns\n",
    "        empty_cols = [col for col in df.columns if df[col].isna().all()]\n",
    "        if empty_cols:\n",
    "            logger.warning(f\"Found completely empty columns in {dataset_id}: {empty_cols}\")\n",
    "        \n",
    "        # Basic check for percentage of missing values\n",
    "        missing_percentage = (df.isna().sum() / len(df) * 100).max()\n",
    "        if missing_percentage > 50:\n",
    "            logger.warning(f\"{dataset_id} has columns with more than 50% missing values (max: {missing_percentage:.2f}%)\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def load_dataset(self, dataset_id: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load all files for a specified dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: ID of the dataset to load\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping file names to DataFrames\n",
    "        \"\"\"\n",
    "        if dataset_id not in self.datasets_config:\n",
    "            logger.error(f\"Unknown dataset ID: {dataset_id}\")\n",
    "            return {}\n",
    "            \n",
    "        config = self.datasets_config[dataset_id]\n",
    "        logger.info(f\"Loading dataset: {config['name']}\")\n",
    "        \n",
    "        # Get all files for this dataset\n",
    "        files = self.get_dataset_files(dataset_id)\n",
    "        if not files:\n",
    "            logger.error(f\"No files found for dataset: {dataset_id}\")\n",
    "            return {}\n",
    "        \n",
    "        result = {}\n",
    "        for file_path in files:\n",
    "            try:\n",
    "                df = self.load_file(dataset_id, file_path)\n",
    "                \n",
    "                # Validate the dataframe\n",
    "                if self.validate_dataframe(df, dataset_id):\n",
    "                    file_name = os.path.basename(file_path)\n",
    "                    result[file_name] = df\n",
    "                else:\n",
    "                    logger.error(f\"Validation failed for {file_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        if not result:\n",
    "            logger.error(f\"Failed to load any files for dataset: {config['name']}\")\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def load_all_datasets(self) -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Load all datasets specified in the configuration.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping dataset IDs to dictionaries mapping file names to DataFrames\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        for dataset_id in self.datasets_config:\n",
    "            dataset_result = self.load_dataset(dataset_id)\n",
    "            if dataset_result:\n",
    "                result[dataset_id] = dataset_result\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2703629-bc30-4b1d-9fc2-e69ce7215dcd",
   "metadata": {},
   "source": [
    "## Data Load Execution\n",
    "\n",
    "Now that we've defined our `DataLoader` class, let's use it to load our datasets. We'll start with a smaller sample size for development purposes, to ensure our code works correctly before scaling up to the full datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8df4a12-64b8-459d-b7d7-b03413212ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 12:34:48,742 - DataAcquisition - INFO - Loading dataset: E-commerce User Behavior Dataset\n",
      "2025-03-26 12:34:48,743 - DataAcquisition - WARNING - No files found matching pattern C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\data\\raw/ecommerce_behavior\\*.csv for dataset ecommerce_behavior\n",
      "2025-03-26 12:34:48,744 - DataAcquisition - ERROR - No files found for dataset: ecommerce_behavior\n",
      "2025-03-26 12:34:48,745 - DataAcquisition - INFO - Loading dataset: Amazon Product Reviews Dataset\n",
      "2025-03-26 12:34:48,746 - DataAcquisition - WARNING - No files found matching pattern C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\data\\raw/amazon_reviews\\*.tsv for dataset amazon_reviews\n",
      "2025-03-26 12:34:48,748 - DataAcquisition - ERROR - No files found for dataset: amazon_reviews\n",
      "2025-03-26 12:34:48,749 - DataAcquisition - INFO - Loading dataset: Online Retail II Dataset\n",
      "2025-03-26 12:34:48,751 - DataAcquisition - WARNING - No files found matching pattern C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\data\\raw/online_retail\\*.xlsx for dataset online_retail\n",
      "2025-03-26 12:34:48,752 - DataAcquisition - ERROR - No files found for dataset: online_retail\n",
      "2025-03-26 12:34:48,753 - DataAcquisition - INFO - Loading dataset: Customer Personality Analysis Dataset\n",
      "2025-03-26 12:34:48,756 - DataAcquisition - WARNING - No files found matching pattern C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\data\\raw/customer_personality\\*.csv for dataset customer_personality\n",
      "2025-03-26 12:34:48,757 - DataAcquisition - ERROR - No files found for dataset: customer_personality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATASETS\n",
      "================================================================================\n",
      "\n",
      "Dataset Loading Summary:\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loader with a sample size for development\n",
    "sample_size = 1000  # Adjust based on your system's memory constraints\n",
    "loader = DataLoader(datasets_config, sample_size=sample_size, cache_enabled=True)\n",
    "\n",
    "# Load all datasets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # This assumes you've already downloaded the datasets to the specified paths\n",
    "    # In a production environment, you'd integrate with a data downloading system or API\n",
    "    all_datasets = loader.load_all_datasets()\n",
    "    \n",
    "    # Print summary of loaded datasets\n",
    "    print(\"\\nDataset Loading Summary:\")\n",
    "    for dataset_id, files_dict in all_datasets.items():\n",
    "        print(f\"\\n{datasets_config[dataset_id]['name']}:\")\n",
    "        if not files_dict:\n",
    "            print(f\"  ❌ No files loaded\")\n",
    "        else:\n",
    "            for file_name, df in files_dict.items():\n",
    "                print(f\"  ✅ {file_name}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "                \n",
    "                # Print first few column names as a preview\n",
    "                print(f\"     Columns: {', '.join(df.columns[:5])}...\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error loading datasets: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c49005-ea0f-4aa0-9be5-57ff93b323dc",
   "metadata": {},
   "source": [
    "## Basic Data Exploration\n",
    "\n",
    "Let's explore each dataset to understand its structure and contents. This will help us identify data cleaning needs and integration points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ba51fd-50bb-4593-8f45-bffeb9209fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(dataset_id: str, files_dict: Dict[str, pd.DataFrame]) -> None:\n",
    "    \"\"\"\n",
    "    Perform basic exploration of a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: ID of the dataset\n",
    "        files_dict: Dictionary mapping file names to DataFrames\n",
    "    \"\"\"\n",
    "    if not files_dict:\n",
    "        print(f\"No data available for {dataset_id}\")\n",
    "        return\n",
    "    \n",
    "    config = datasets_config[dataset_id]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPLORING {config['name'].upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for file_name, df in files_dict.items():\n",
    "        print(f\"\\nFile: {file_name}\")\n",
    "        \n",
    "        # 1. Basic information\n",
    "        print(f\"\\nShape: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "        # 2. Data types\n",
    "        print(\"\\nData Types:\")\n",
    "        for dtype, count in df.dtypes.value_counts().items():\n",
    "            print(f\"  - {dtype}: {count} columns\")\n",
    "        \n",
    "        # 3. Missing values\n",
    "        missing = df.isna().sum()\n",
    "        missing_cols = missing[missing > 0]\n",
    "        print(\"\\nMissing Values:\")\n",
    "        if len(missing_cols) == 0:\n",
    "            print(\"  - No missing values\")\n",
    "        else:\n",
    "            for col, count in missing_cols.items():\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"  - {col}: {count:,} missing values ({percentage:.2f}%)\")\n",
    "        \n",
    "        # 4. Sample data\n",
    "        print(\"\\nSample Data:\")\n",
    "        display(df.head(3))\n",
    "        \n",
    "        # 5. Dataset-specific exploration\n",
    "        if dataset_id == \"ecommerce_behavior\":\n",
    "            # Check event types distribution\n",
    "            if 'event_type' in df.columns:\n",
    "                print(\"\\nEvent Type Distribution:\")\n",
    "                event_counts = df['event_type'].value_counts()\n",
    "                for event, count in event_counts.items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"  - {event}: {count:,} events ({percentage:.2f}%)\")\n",
    "            \n",
    "            # Check top categories\n",
    "            if 'category_id' in df.columns:\n",
    "                print(\"\\nTop 5 Categories:\")\n",
    "                cat_counts = df['category_id'].value_counts().head(5)\n",
    "                for cat, count in cat_counts.items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"  - {cat}: {count:,} events ({percentage:.2f}%)\")\n",
    "                    \n",
    "        elif dataset_id == \"amazon_reviews\":\n",
    "            # Check rating distribution\n",
    "            if 'star_rating' in df.columns:\n",
    "                print(\"\\nRating Distribution:\")\n",
    "                rating_counts = df['star_rating'].value_counts().sort_index()\n",
    "                for rating, count in rating_counts.items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"  - {rating} stars: {count:,} reviews ({percentage:.2f}%)\")\n",
    "            \n",
    "            # Check product categories\n",
    "            if 'product_category' in df.columns:\n",
    "                print(\"\\nTop 5 Product Categories:\")\n",
    "                cat_counts = df['product_category'].value_counts().head(5)\n",
    "                for cat, count in cat_counts.items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"  - {cat}: {count:,} reviews ({percentage:.2f}%)\")\n",
    "                    \n",
    "        elif dataset_id == \"online_retail\":\n",
    "            # Check top countries\n",
    "            if 'Country' in df.columns:\n",
    "                print(\"\\nTop 5 Countries:\")\n",
    "                country_counts = df['Country'].value_counts().head(5)\n",
    "                for country, count in country_counts.items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"  - {country}: {count:,} transactions ({percentage:.2f}%)\")\n",
    "            \n",
    "            # Check transaction distribution over time\n",
    "            if 'InvoiceDate' in df.columns:\n",
    "                print(\"\\nTransaction Timeline:\")\n",
    "                df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
    "                month_counts = df['InvoiceMonth'].value_counts().sort_index()\n",
    "                for month, count in month_counts.head(5).items():\n",
    "                    print(f\"  - {month}: {count:,} transactions\")\n",
    "                    \n",
    "        elif dataset_id == \"customer_personality\":\n",
    "            # Check demographics\n",
    "            if 'Education' in df.columns:\n",
    "                print(\"\\nEducation Distribution:\")\n",
    "                edu_counts = df['Education'].value_counts()\n",
    "                for edu, count in edu_counts.items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"  - {edu}: {count:,} customers ({percentage:.2f}%)\")\n",
    "            \n",
    "            # Check age distribution\n",
    "            if 'Year_Birth' in df.columns:\n",
    "                print(\"\\nAge Distribution:\")\n",
    "                current_year = datetime.now().year\n",
    "                df['Age'] = current_year - df['Year_Birth']\n",
    "                print(f\"  - Age range: {df['Age'].min()} to {df['Age'].max()} years\")\n",
    "                print(f\"  - Average age: {df['Age'].mean():.1f} years\")\n",
    "                print(f\"  - Median age: {df['Age'].median():.1f} years\")\n",
    "        \n",
    "        # 6. Memory usage\n",
    "        memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)  # in MB\n",
    "        print(f\"\\nMemory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "# Explore each dataset\n",
    "for dataset_id, files_dict in all_datasets.items():\n",
    "    explore_dataset(dataset_id, files_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31922a3f-5750-431e-85e2-0f4062c1f2dc",
   "metadata": {},
   "source": [
    "## Initial Data Quality Assessment\n",
    "\n",
    "Based on our exploration, let's identify key data quality issues that we'll need to address in the data cleaning notebook. This will help us plan our data preprocessing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d810d4-ed6d-4c72-8cfe-fc2bd3ba14e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def assess_data_quality(all_datasets: Dict[str, Dict[str, pd.DataFrame]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Assess data quality issues in all datasets.\n",
    "    \n",
    "    Args:\n",
    "        all_datasets: Dictionary of all loaded datasets\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping dataset IDs to lists of data quality issues\n",
    "    \"\"\"\n",
    "    quality_issues = {}\n",
    "    \n",
    "    for dataset_id, files_dict in all_datasets.items():\n",
    "        issues = []\n",
    "        \n",
    "        # Skip if no files were loaded\n",
    "        if not files_dict:\n",
    "            quality_issues[dataset_id] = [\"No files loaded\"]\n",
    "            continue\n",
    "        \n",
    "        # Take the first DataFrame for assessment\n",
    "        df = next(iter(files_dict.values()))\n",
    "        \n",
    "        # 1. Check for missing values\n",
    "        missing_cols = df.columns[df.isna().mean() > 0.05]\n",
    "        if len(missing_cols) > 0:\n",
    "            issues.append(f\"High missing values (>5%) in columns: {', '.join(missing_cols)}\")\n",
    "        \n",
    "        # 2. Check for duplicates\n",
    "        try:\n",
    "            dup_count = df.duplicated().sum()\n",
    "            dup_pct = (dup_count / len(df)) * 100\n",
    "            if dup_pct > 0.1:\n",
    "                issues.append(f\"Contains {dup_count:,} duplicated rows ({dup_pct:.2f}%)\")\n",
    "        except:\n",
    "            issues.append(\"Could not check for duplicates (possibly due to non-hashable types)\")\n",
    "        \n",
    "        # 3. Dataset-specific checks\n",
    "        if dataset_id == \"ecommerce_behavior\":\n",
    "            # Check for invalid event types\n",
    "            if 'event_type' in df.columns:\n",
    "                valid_events = ['view', 'cart', 'purchase', 'remove_from_cart']\n",
    "                invalid_events = df['event_type'].unique().tolist()\n",
    "                invalid_events = [e for e in invalid_events if e not in valid_events]\n",
    "                if invalid_events:\n",
    "                    issues.append(f\"Contains invalid event types: {invalid_events}\")\n",
    "            \n",
    "            # Check for reasonable price ranges\n",
    "            if 'price' in df.columns:\n",
    "                if df['price'].min() < 0:\n",
    "                    issues.append(\"Contains negative prices\")\n",
    "                if df['price'].max() > 100000:\n",
    "                    issues.append(\"Contains suspiciously high prices (>$100,000)\")\n",
    "        \n",
    "        elif dataset_id == \"amazon_reviews\":\n",
    "            # Check for invalid ratings\n",
    "            if 'star_rating' in df.columns:\n",
    "                invalid_ratings = df['star_rating'][(df['star_rating'] < 1) | (df['star_rating'] > 5)].count()\n",
    "                if invalid_ratings > 0:\n",
    "                    issues.append(f\"Contains {invalid_ratings:,} invalid ratings (not 1-5)\")\n",
    "        \n",
    "        elif dataset_id == \"online_retail\":\n",
    "            # Check for negative quantities\n",
    "            if 'Quantity' in df.columns:\n",
    "                neg_qty = (df['Quantity'] < 0).sum()\n",
    "                if neg_qty > 0:\n",
    "                    neg_pct = (neg_qty / len(df)) * 100\n",
    "                    issues.append(f\"Contains {neg_qty:,} negative quantities ({neg_pct:.2f}%)\")\n",
    "            \n",
    "            # Check for negative prices\n",
    "            if 'UnitPrice' in df.columns:\n",
    "                neg_price = (df['UnitPrice'] < 0).sum()\n",
    "                if neg_price > 0:\n",
    "                    issues.append(f\"Contains {neg_price:,} negative unit prices\")\n",
    "        \n",
    "        elif dataset_id == \"customer_personality\":\n",
    "            # Check for invalid age values\n",
    "            if 'Year_Birth' in df.columns:\n",
    "                current_year = datetime.now().year\n",
    "                invalid_age = ((current_year - df['Year_Birth']) > 100).sum()\n",
    "                if invalid_age > 0:\n",
    "                    issues.append(f\"Contains {invalid_age:,} customers with age > 100 years\")\n",
    "        \n",
    "        # If no issues were found, add a positive note\n",
    "        if not issues:\n",
    "            issues.append(\"No major data quality issues detected\")\n",
    "        \n",
    "        quality_issues[dataset_id] = issues\n",
    "    \n",
    "    return quality_issues\n",
    "\n",
    "# Assess data quality\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "quality_issues = assess_data_quality(all_datasets)\n",
    "\n",
    "for dataset_id, issues in quality_issues.items():\n",
    "    print(f\"\\n{datasets_config[dataset_id]['name']}:\")\n",
    "    for issue in issues:\n",
    "        issue_symbol = \"❌\" if \"No major\" not in issue else \"✅\"\n",
    "        print(f\"  {issue_symbol} {issue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437671d8-627e-4f32-b50b-9d73854109aa",
   "metadata": {},
   "source": [
    "## Data Integration Strategy\n",
    "\n",
    "Let's briefly outline our strategy for integrating these datasets in the next notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8662b1b1-95ca-4db1-abde-66131cc08bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA INTEGRATION STRATEGY\n",
      "================================================================================\n",
      "\n",
      "Our data integration approach will focus on:\n",
      "\n",
      "1. **Customer Entity Resolution**\n",
      "   * Create unified customer profiles by matching identifiers across datasets\n",
      "   * Use demographic information where available to enhance matching\n",
      "\n",
      "2. **Product Entity Resolution**\n",
      "   * Map products between E-commerce Behavior and Amazon Reviews datasets\n",
      "   * Use product attributes and categories for matching\n",
      "\n",
      "3. **Key Integration Points**\n",
      "   * User behavior → Customer profile linkage\n",
      "   * Product interactions → Product review sentiment\n",
      "   * Purchase history → Customer value metrics\n",
      "   * Demographics → Behavioral segmentation\n",
      "\n",
      "This integrated data will form the foundation for our recommendation models, \n",
      "churn prediction, and customer lifetime value analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA INTEGRATION STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Our data integration approach will focus on:\n",
    "\n",
    "1. **Customer Entity Resolution**\n",
    "   * Create unified customer profiles by matching identifiers across datasets\n",
    "   * Use demographic information where available to enhance matching\n",
    "\n",
    "2. **Product Entity Resolution**\n",
    "   * Map products between E-commerce Behavior and Amazon Reviews datasets\n",
    "   * Use product attributes and categories for matching\n",
    "\n",
    "3. **Key Integration Points**\n",
    "   * User behavior → Customer profile linkage\n",
    "   * Product interactions → Product review sentiment\n",
    "   * Purchase history → Customer value metrics\n",
    "   * Demographics → Behavioral segmentation\n",
    "\n",
    "This integrated data will form the foundation for our recommendation models, \n",
    "churn prediction, and customer lifetime value analysis.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef075f0f-2290-40cb-acda-8ef9a8523658",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Let's summarize what we've accomplished in this notebook and outline the next steps in our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11eb3356-afa4-49d2-82ad-9734092caa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY AND NEXT STEPS\n",
      "================================================================================\n",
      "\n",
      "## What We've Accomplished\n",
      "\n",
      "In this notebook, we've:\n",
      "\n",
      "1. 📂 Created a structured project organization with 9 directories\n",
      "2. 📊 Configured 4 datasets containing valuable customer behavior data\n",
      "3. 🔄 Implemented a robust data loading framework with caching and validation\n",
      "4. 📋 Loaded 0 data files containing 0 total rows\n",
      "5. 🔍 Explored the key characteristics of each dataset\n",
      "6. 🚩 Identified data quality issues to address in preprocessing\n",
      "7. 🗺️ Outlined a strategy for integrating these datasets\n",
      "\n",
      "## Next Steps\n",
      "\n",
      "1. **Notebook 2: Data Cleaning and Preprocessing**\n",
      "   * Address the identified data quality issues\n",
      "   * Handle missing values and outliers\n",
      "   * Standardize formats and units across datasets\n",
      "   * Create a clean, consistent foundation for analysis\n",
      "\n",
      "2. **Notebook 3: Exploratory Data Analysis**\n",
      "   * Discover patterns in customer behavior\n",
      "   * Identify correlations between features\n",
      "   * Generate insights to inform model development\n",
      "   * Visualize key relationships in the data\n",
      "\n",
      "3. **Notebook 4: Feature Engineering**\n",
      "   * Create new features that capture behavior patterns\n",
      "   * Develop time-based features to capture trends\n",
      "   * Implement the integration strategy outlined above\n",
      "   * Prepare features specifically for recommendation models\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def print_summary():\n",
    "    \"\"\"\n",
    "    Print a summary of the notebook and next steps.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY AND NEXT STEPS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Count total rows across all datasets\n",
    "    total_rows = 0\n",
    "    loaded_files = 0\n",
    "    \n",
    "    for dataset_id, files_dict in all_datasets.items():\n",
    "        for file_name, df in files_dict.items():\n",
    "            total_rows += len(df)\n",
    "            loaded_files += 1\n",
    "    \n",
    "    print(f\"\"\"\n",
    "## What We've Accomplished\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. 📂 Created a structured project organization with {len(project_dirs)} directories\n",
    "2. 📊 Configured {len(datasets_config)} datasets containing valuable customer behavior data\n",
    "3. 🔄 Implemented a robust data loading framework with caching and validation\n",
    "4. 📋 Loaded {loaded_files} data files containing {total_rows:,} total rows\n",
    "5. 🔍 Explored the key characteristics of each dataset\n",
    "6. 🚩 Identified data quality issues to address in preprocessing\n",
    "7. 🗺️ Outlined a strategy for integrating these datasets\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Notebook 2: Data Cleaning and Preprocessing**\n",
    "   * Address the identified data quality issues\n",
    "   * Handle missing values and outliers\n",
    "   * Standardize formats and units across datasets\n",
    "   * Create a clean, consistent foundation for analysis\n",
    "\n",
    "2. **Notebook 3: Exploratory Data Analysis**\n",
    "   * Discover patterns in customer behavior\n",
    "   * Identify correlations between features\n",
    "   * Generate insights to inform model development\n",
    "   * Visualize key relationships in the data\n",
    "\n",
    "3. **Notebook 4: Feature Engineering**\n",
    "   * Create new features that capture behavior patterns\n",
    "   * Develop time-based features to capture trends\n",
    "   * Implement the integration strategy outlined above\n",
    "   * Prepare features specifically for recommendation models\n",
    "    \"\"\")\n",
    "\n",
    "# Print summary\n",
    "print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a917027c-6f12-48ad-b865-99c199664603",
   "metadata": {},
   "source": [
    "## Environment Information\n",
    "\n",
    "For reproducibility, let's save information about the environment where this notebook was executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cefca284-7fa0-48ed-bae3-a30c883e5698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment Information:\n",
      "  - python_version: 3.11.5\n",
      "  - platform: Windows-10-10.0.26100-SP0\n",
      "  - pandas_version: 2.1.1\n",
      "  - numpy_version: 1.26.0\n",
      "  - execution_date: 2025-03-26 12:35:10\n",
      "\n",
      "Environment information saved to C:\\_Arash\\github\\ecom-reco-predictor\\full_recom_prediction_engine\\ecommerce_recommendation_project\\logs/environment_info.json\n"
     ]
    }
   ],
   "source": [
    "def save_environment_info():\n",
    "    \"\"\"\n",
    "    Save information about the execution environment for reproducibility.\n",
    "    \"\"\"\n",
    "    import platform\n",
    "    import sys\n",
    "    \n",
    "    env_info = {\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"pandas_version\": pd.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"execution_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(f\"{project_dirs['logs']}/environment_info.json\", 'w') as f:\n",
    "        json.dump(env_info, f, indent=4)\n",
    "    \n",
    "    print(\"\\nEnvironment Information:\")\n",
    "    for key, value in env_info.items():\n",
    "        print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nEnvironment information saved to {project_dirs['logs']}/environment_info.json\")\n",
    "\n",
    "# Save environment information\n",
    "save_environment_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7fea4-85ae-406c-9509-6866c18f1cde",
   "metadata": {},
   "source": [
    "This completes our Data Acquisition and Integration notebook. We've set up a robust foundation for our E-commerce Customer Behavior Prediction and Recommendation Engine project. In the next notebook, we'll focus on cleaning and preprocessing the data to address the quality issues we've identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2326610-b9f2-45f7-b4c0-6923a18c95ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
